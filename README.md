# Local LLM Chatbot with Advanced RAG Capabilities

This project is a **fully local document chatbot** powered by Large Language Models (LLMs). It allows users to upload PDFs and ask questions â€” all without sending data to external servers.

---

## âœ¨ Features

- **Local LLM Inference**: Runs quantized `.gguf` models (e.g. Mistral) using `llama.cpp`
- **PDF Upload & Chunking**: Parses PDFs into context-aware chunks for retrieval
- **Vector Search**: Embeds chunks using SentenceTransformers & retrieves top-K relevant passages
- **Knowledge Graph Integration**: Uses Neo4j to capture entities and relationships between concepts
- **Chain-of-Thought Reasoning**: LLM breaks down steps before answering
- **Multi-hop Reasoning**: Handles complex queries by chaining sub-questions
- **Answer Verification**: Validates responses against retrieved sources

---

## ğŸ§  Architecture

```
User â”€â”¬â”€â–¶ Upload PDF â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â–¶ Document Processor
     â”‚                   â”‚
     â”‚                   â””â”€â”€â”€â”€â–¶ Chunk & Embed
     â”‚                           â”‚
     â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
     â”‚                 â–¼               â–¼
     â”‚         Vector Store         Knowledge Graph
     â”‚             â”‚                     â”‚
     â”‚             â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                  â–¼    â–¼
     â”‚           Hybrid Retriever
     â”‚                  â”‚
     â”‚          Chain-of-Thought
     â”‚                  â”‚
     â”‚             Final Answer
     â–¼
   Chat UI (Electron)
```

---

## ğŸ—‚ï¸ Key Components

| File | Purpose |
|------|---------|
| `app_integration.py` | FastAPI server wiring all components |
| `llm_interface.py` | Loads GGUF model using llama.cpp via `ctypes` |
| `vector_store.py` | Chroma vector DB wrapper with embedding logic |
| `knowledge_graph.py` | spaCy + Neo4j entity & relation extractor |
| `chain_of_thought.py` | Implements CoT + reasoning parsing |
| `document_processor.py` | PDF chunking, cleaning, and metadata attachment |
| `frontend/` | Electron + Vite React app to interact with backend |

---

## ğŸš€ Getting Started

### âœ… Prerequisites

- Python 3.10+
- Node.js 18+
- Neo4j installed locally
- `llama.cpp` compiled as shared library (`libllama.dylib`)
- A `.gguf` model like `mistral-7b-instruct-v0.1.Q4_K_M.gguf`

### ğŸ› ï¸ Setup (Backend)

```bash
# Clone the repo
git clone https://github.com/yourname/local-llm-chatbot.git
cd local-llm-chatbot/backend

# Install Python deps
pip install -r requirements.txt

# Set Neo4j credentials
export NEO4J_USERNAME=neo4j
export NEO4J_PASSWORD=your_password

# Start backend
python app_integration.py
```

### ğŸ–¥ï¸ Setup (Frontend)

```bash
cd ../frontend
npm install
npm run dev  # or: npm run build && npm start
```

---

## ğŸ’¬ API Quick Reference

| Endpoint | Description |
|----------|-------------|
| `POST /upload` | Upload and process a PDF |
| `POST /query` | Ask a question using hybrid RAG |
| `GET /models` | List available local models |
| `GET /advanced-rag/features` | View enabled features |

---

## ğŸ§ª Example Usage

### Python
```python
from advanced_rag import AdvancedRAG

rag = AdvancedRAG(...)
rag.add_documents([doc1, doc2])
result = rag.answer_query("What is the main finding?")
print(result["answer"])
```

### Curl
```bash
curl -X POST -F "file=@report.pdf" http://localhost:8000/upload

curl -X POST -H "Content-Type: application/json" \
     -d '{"query": "What is the report summary?"}' \
     http://localhost:8000/query
```

---

## ğŸ› ï¸ Configuration Flags

| Flag | Description | Default |
|------|-------------|---------|
| `use_cot` | Enable step-by-step reasoning | `True` |
| `use_kg` | Enable Neo4j knowledge graph | `True` |
| `use_multihop` | Break down complex queries | `True` |
| `verify_answers` | Verify answer against sources | `True` |
| `temperature` | Model creativity level | `0.7` |

---

## ğŸ“ˆ Performance Tips

- Run models with 4-bit quantization (`Q4_K_M`) for faster inference
- Use ChromaDB with persistence for large corpora
- Enable `llama.cpp` Metal backend for M1/M2 Macs
- Pre-index documents for better first-response latency

---

## ğŸš§ Limitations

- No GPU acceleration (yet)
- spaCy extraction may miss domain-specific terms
- CoT & multihop increase latency
- Basic Electron UI (for now)



---

## ğŸªª License

MIT License

