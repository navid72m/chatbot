# Document Chat â€” Localâ€‘First Fullâ€‘Stack RAG (Electron + FastAPI + React)

A productionâ€‘oriented **local document chat** system powered by **GGUF local LLMs** (llama.cpp), **hybrid retrieval** (Vector + BM25), optional **reranking**, and a **navigationâ€‘aware RAG layer** for queries like:

- **â€œSummarize page 12â€**
- **â€œWhat is Chapter 8 about?â€**

This repo is designed as a complete app, not a notebook demo:
- **Frontend:** React (chat UI)
- **Desktop:** Electron (packaged app)
- **Backend:** FastAPI (document processing + RAG)
- **Retrieval:** Vector search + BM25 + RRF fusion
- **Optional:** Crossâ€‘encoder reranking
- **Evaluation:** recall/precision/MRR/NDCG/MAP + latency

---

## ğŸ¥ Demo

[Demo video](https://youtu.be/cIJL3SNN4R4)

---

## âœ¨ Key Features

### Core
- Upload and chat with documents (PDF, images, text, JSON/CSV)
- **Localâ€‘first**: run fully offline (privacyâ€‘friendly)
- OCR support for scanned PDFs/images (PaddleOCR)
- Documentâ€‘grounded answers with cited snippets
- Suggested questions after upload

### Retrieval & Ranking
- **Hybrid retrieval**:
  - Vector search (semantic)
  - BM25 search (lexical)
  - RRF fusion (robust & stable)
- Optional reranking (crossâ€‘encoder)
- **Tokenâ€‘budgeted context builder** (prevents context overflow)

### Navigationâ€‘Aware RAG (Important)
Classic RAG often fails on navigation queries (â€œpage/chapter/sectionâ€), because retrieval is semantic.

This repo includes **intent routing** that detects such queries and fetches content **directly by page/chapter**, bypassing retrieval when appropriate.

---

## ğŸ§  Architecture

### RAG Pipeline Diagram

> Save your diagram image as: `assets/rag_pipeline.png`

![Enhanced RAG Pipeline](assets/rag_pipeline.png)

---

## ğŸ” How It Works (Short)

### Offline indexing
1. Extract text (OCR if needed)
2. Split into chunks + metadata (page, offsets)
3. Compute embeddings
4. Build indexes:
   - Vector index (FAISS)
   - BM25 index
   - Metadata store

### Online query
1. **Intent detection**
2. Route query:
   - **Semantic QA** â†’ Hybrid Retrieval â†’ (Optional rerank)
   - **Page/Chapter** â†’ Direct chunk fetch (skip rerank)
3. Build context within token budget
4. Generate grounded answer with LLM

---

## ğŸ§© Repository Structure (Typical)

```text
.
â”œâ”€â”€ backend/                 # FastAPI backend
â”‚   â”œâ”€â”€ app_integration_updated.py
â”‚   â”œâ”€â”€ document_processor.py
â”‚   â”œâ”€â”€ models/              # GGUF models
â”‚   â””â”€â”€ ...
â”œâ”€â”€ frontend/                # React UI
â”œâ”€â”€ electron/                # Electron packaging
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ rag_pipeline.png     # README diagram
â””â”€â”€ fast_rag_evaluation.py   # eval runner
```

---

## ğŸ§° Tech Stack

- **Backend:** FastAPI, Python
- **LLM:** llama.cpp (GGUF)
- **Embeddings:** sentence-transformers
- **Vector Index:** FAISS
- **Lexical Search:** BM25
- **Reranking (optional):** crossâ€‘encoder / BGE reranker
- **Frontend:** React
- **Desktop:** Electron

---

## ğŸš€ Quickstart

### 1) Clone
```bash
git clone https://github.com/navid72m/pdf.git
cd pdf
```

### 2) Backend setup
```bash
cd backend
python -m venv .venv
source .venv/bin/activate

pip install -r requirements.txt
```

If you use OCR:
```bash
pip install paddleocr
```

### 3) Add a GGUF model
Put your model into:
```bash
backend/models/
```

Example:
```text
backend/models/deepseek-r1.Q4_K_M.gguf
```

### 4) Start backend
```bash
python app_integration_updated.py --host 127.0.0.1 --port 8000
```

### 5) Start frontend
```bash
cd ../frontend
npm install
npm run dev
```

(Optional) Electron desktop:
```bash
cd ../electron
npm install
npm start
```

---

## âš™ï¸ Configuration

Common env vars:

```bash
export LLAMA_CPP_MODEL_PATH="./backend/models/deepseek-r1.Q4_K_M.gguf"
export LLAMA_CTX_SIZE=4096
export LLAMA_THREADS=8
```

---

## ğŸ”Œ Backend API

### Upload document
```http
POST /upload
Content-Type: multipart/form-data
```

### Query document
```http
POST /query
Content-Type: application/json

{
  "query": "Summarize page 12",
  "document": "myfile.pdf"
}
```

### Suggested questions
```http
GET /suggestions?document=myfile.pdf
```

---

## ğŸ§ª Evaluation

This repo includes evaluation scripts to compare:

- baseline hybrid retrieval
- enhanced navigationâ€‘aware retrieval

Metrics:
- Recall@K / Precision@K
- MRR / NDCG / MAP
- Latency (mean / p95 / p99)

Run:
```bash
python fast_rag_evaluation.py
```

---

## ğŸ› ï¸ Troubleshooting

### Context window exceeded
Solutions:
- cap context tokens (e.g., 2500â€“5000)
- rerank only top 10 candidates
- reduce neighbor expansion
- shorten chunk size

### Enhanced pipeline too slow
- navigation queries should **skip reranking**
- cache embeddings + retrieval results
- rerank only top N (10)

---

## ğŸ—ºï¸ Roadmap

- [ ] Better chapter detection via TOC parsing
- [ ] Hard negative dataset generation for eval
- [ ] Multiâ€‘document workspace
- [ ] Streaming UI improvements
- [ ] Benchmark embedding/reranker choices

---

## ğŸ¤ Contributing

PRs welcome.  
If you change retrieval logic, please attach:
- eval results
- latency impact

---

## ğŸ“„ License

MIT
