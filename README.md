# ğŸ§  Local LLM Desktop Chatbot with Advanced RAG & Ollama Integration

This project is a **fully local desktop application** that enables secure, offline document-based chat using Large Language Models (LLMs). It uses **Ollama** to run quantized models (like `deepseek-coder` or `mistral`) and supports advanced Retrieval-Augmented Generation (RAG) features such as:

- ğŸ§¾ Multi-format document upload
- ğŸ” Vector + knowledge graph-based hybrid search
- ğŸ”„ Chain-of-thought reasoning
- âœ… Answer verification against retrieved context

---

## âœ¨ Key Features

- **ğŸ” 100% Local Inference** via [Ollama](https://ollama.com)
- **ğŸ“„ Multi-format Upload**: PDF, DOCX, TXT, CSV, XLSX
- **ğŸ“š Hybrid RAG**: Combines vector search (ChromaDB) + Neo4j knowledge graph
- **ğŸ§  CoT & Multihop Reasoning**: Breaks down complex queries
- **ğŸ“ Answer Verification**: Validates LLM output against sources
- **ğŸ§‘â€ğŸ’» Electron App**: Cross-platform frontend (macOS, Windows)

---

## ğŸ§± Architecture Diagram

```
User â”€â”¬â”€â–¶ Upload PDF â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â–¶ Document Processor
     â”‚                   â”‚
     â”‚                   â””â”€â”€â”€â”€â–¶ Chunk & Embed
     â”‚                           â”‚
     â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
     â”‚                 â–¼               â–¼
     â”‚         Vector Store         Knowledge Graph
     â”‚             â”‚                     â”‚
     â”‚             â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                  â–¼    â–¼
     â”‚           Hybrid Retriever
     â”‚                  â”‚
     â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚       â–¼                       â–¼
     â”‚   Answer Verifier        Chain-of-Thought
     â”‚       â”‚                       â”‚
     â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                  â–¼
     â”‚          Local LLM via Ollama
     â–¼
   Electron Desktop App
```

---

## ğŸ—‚ï¸ Key Components

| File / Dir | Description |
|------------|-------------|
| `backend/` | FastAPI + Python backend with RAG logic |
| `electron/` | Electron main process, starts backend & Ollama |
| `frontend/` | Vite + React UI for chatting, uploading |
| `ollama/` | Local LLM runner and model container |
| `Modelfile` | Ollama configuration file (base model) |

---

## ğŸ§ª Tech Stack

- ğŸ§  **LLM Inference**: [Ollama](https://ollama.com) with quantized `.gguf` models
- ğŸ§¾ **Document Parsing**: PyMuPDF, Pandas
- ğŸ’¡ **Embeddings**: `sentence-transformers`
- ğŸ” **Vector DB**: ChromaDB
- ğŸ§¬ **Knowledge Graph**: spaCy + Neo4j
- ğŸ§© **Frontend**: React + Vite (Electron)
- ğŸ **Backend**: FastAPI with modular RAG pipeline

---

## ğŸš€ Getting Started

### âœ… Prerequisites

- Python 3.10+
- Node.js 18+
- Neo4j running locally
- Ollama installed (or embedded binary in `electron/bin/ollama`)

### ğŸ› ï¸ Backend Setup

```bash
cd backend
python -m venv venv && source venv/bin/activate
pip install -r requirements.txt
export NEO4J_USERNAME=neo4j
export NEO4J_PASSWORD=your_password
python app.py  # Or use compiled backend binary
```

### ğŸ–¥ï¸ Frontend & Electron App

```bash
cd frontend
npm install
npm run build  # Build static frontend assets

# In project root
npm run electron:dev  # For dev
npm run electron:build  # For packaged macOS/Windows app
```

---

## ğŸ“¦ Packaging for Distribution

Ensure `electron/bin/backend` and `electron/bin/ollama` exist and are executable.

```bash
npm run electron:build  # Generates DMG/EXE in /release/
```

---

## ğŸ§ª API Reference

| Endpoint | Description |
|----------|-------------|
| `GET /health` | Check server health |
| `POST /upload` | Upload and embed document |
| `POST /query` | Ask a question to LLM |
| `POST /configure` | Change reasoning settings |
| `GET /models` | List supported local models |

---

## âš™ï¸ Configuration Flags

| Flag | Description | Default |
|------|-------------|---------|
| `use_kg` | Enable Neo4j knowledge graph | `True` |
| `use_cot` | Enable chain-of-thought | `True` |
| `use_multihop` | Multi-hop retrieval | `True` |
| `verify_answers` | Answer validation | `True` |
| `temperature` | Model creativity | `0.7` |
| `context_window` | How many chunks to pass to model | `10` |

---

## ğŸ› ï¸ Troubleshooting

- **App launches but shows blank screen**: Check index.html paths in `vite.config.js`
- **Backend not connecting**: Ensure the health check route `/health` returns `200`
- **Port conflicts**: Make sure port 8000 isnâ€™t already in use
- **Error: spawn backend ENOENT**: Ensure binary exists and is executable

---

## ğŸ“ˆ Performance Tips

- Use 4-bit quantized models like `mistral.Q4_K_M`
- Pre-warm documents into ChromaDB
- Set `Ollama` to use Metal backend (for macOS M1/M2)
- Reduce `context_window` to speed up answers

---

## ğŸ“œ License

MIT License â€” All documents stay local.

---

Let me know if you'd like me to embed actual screenshots, diagnostic logs, or contribution guidelines.